{
 "metadata": {
  "name": "",
  "signature": "sha256:91a20aaf07f66bec6dbbdc74293e552481d395019976834c85b981338e316980"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import scipy.spatial.distance as dist\n",
      "from sklearn.metrics import accuracy_score\n",
      "from collections import Counter\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "import pylab as pl"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "K-Nearest Neighbors"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Q1 Implement a function that does kNN classification, and use it to classify the Iris dataset. Once you get a near-perfect classification there, use your function in the 3DClothing dataset. Plot the accuracy for all odd values of k from 1 to 9.\n",
      "\n",
      "        Hint: the cdist function in SciPy may be helpful.\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = np.array([map(float, x.split(',')[:-1]) for x in open('iris.data') if x.strip()!=''])\n",
      "labels = np.array([x.split(',')[-1].strip() for x in open('iris.data') if x.strip()!=''])\n",
      "\n",
      "idx_train = np.loadtxt('iris_idx_train.txt')\n",
      "idx_test = np.loadtxt('iris_idx_test.txt')\n",
      "\n",
      "idx_train = idx_train.astype(int)\n",
      "idx_test = idx_test.astype(int)\n",
      "\n",
      "data_training = data[idx_train,:]\n",
      "data_testing = data[idx_test,:]\n",
      "labels_training = labels[idx_train]\n",
      "labels_testing = labels[idx_test]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def kNN(data_training, data_testing, labels_training, labels_testing):\n",
      "    for i in range(1,9):\n",
      "        distance = dist.cdist(data_training, data_testing)\n",
      "        min_k = np.argsort(distance.T,1)[:,1:i+1]\n",
      "        min_labels = labels_training[min_k]\n",
      "        accuracy = accuracy_score(labels_testing, [Counter(x).most_common()[0][0] for x in min_labels] )\n",
      "        print accuracy\n",
      "\n",
      "kNN (data_training, data_testing, labels_training, labels_testing)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Logistic Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Q2 First we will focus on a two-class problem. Use slicing to get a new training and testing set that only contains the instances corresponding to shirt and jeans (remember to also create new label variables!). Then, train a Logistic Regression classifier, adjusting the C parameter with cross-validation. This time you can chose to use the cross-validation functions provided by sklearn. Plot the training and validation accuracy as C is incresed, and print the test accuracy for the selected model.\n",
      "\n",
      "    Let your C search range be from 10^-7 to 10^7.\n",
      "    Hint: use log-scale for the C value in the plot. Hint: Since we do not have a lot of training data, use 15 folds to ensure the train set will be large enough.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A = list(range(1, 16))\n",
      "B = list(range(1, 16))\n",
      "p = 0\n",
      "data_training = np.load('3dclothing_train.npy')\n",
      "data_testing = np.load('3dclothing_test.npy')\n",
      "\n",
      "labels_training = np.array([x.strip() for x in open('3dclothing_labels_train.txt')])\n",
      "labels_testing = np.array([x.strip() for x in open('3dclothing_labels_test.txt')])\n",
      "\n",
      "ok_data_training = data_training[(labels_training == 'shirt') + (labels_training == 'jeans'),:]\n",
      "ok_data_testing = data_testing[(labels_testing == 'shirt') + (labels_testing == 'jeans'),:]\n",
      "\n",
      "ok_labels_training = labels_training[(labels_training == 'shirt') + (labels_training == 'jeans')]\n",
      "ok_labels_testing = labels_testing[(labels_testing == 'shirt') + (labels_testing == 'jeans')]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in xrange (-7,8):    \n",
      "    LogReg = LogisticRegression(C=10**i)\n",
      "    A[p]=i\n",
      "    LogReg.fit(ok_data_training, ok_labels_training)\n",
      "    B[p] = LogReg.score(ok_data_testing, ok_labels_testing)\n",
      "    print 'Logistic Regression accuracy for C = 10^', i, ' is ', B[p]\n",
      "    p+=1\n",
      "\n",
      "pl.plot(A[:14],B[:14])\n",
      "\n",
      "pl.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Q4 Sometimes we want to re-use a trained classified in another system, or with another programming language. Other times we just want to save it to disk for later usage. Given a trained linear classifier object, explain what information should we be saving in order to be able to do so. Next, inspect the \"shirts vs jeans\" logistic regression classifier object we have trained in Question 2 (re-train it if necessary) and identify which variables contain said information. Finally, write the code necessary to use these variables to classify new test samples.\n",
      "\n",
      "    Hint: In Python, you can see the methods and variables of an object with dir(object).\n",
      "    Hint: Variables that start and end with two underscores, such as \"__dir__\" are internal Python object methods, not relevant for our problem.\n",
      "    Hint: In sklearn, variables tend to have a underscore at the end, while functions have no underscores at the beginning/ending of the name. Remember that the equation for Logistic Regression is:\n",
      "    logreg\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_training = np.load('3dclothing_train.npy')\n",
      "data_testing = np.load('3dclothing_test.npy')\n",
      "\n",
      "labels_training = np.array([x.strip() for x in open('3dclothing_labels_train.txt')])\n",
      "labels_testing = np.array([x.strip() for x in open('3dclothing_labels_test.txt')])\n",
      "\n",
      "ok_data_testing = data_testing[(labels_testing == 'shirt') + (labels_testing == 'jeans'),:]\n",
      "ok_data_training = data_training[(labels_training == 'shirt') + (labels_training == 'jeans'),:]\n",
      "\n",
      "ok_labels_training = labels_training[(labels_training == 'shirt') + (labels_training == 'jeans')]\n",
      "ok_labels_testing = labels_testing[(labels_testing == 'shirt') + (labels_testing == 'jeans')]\n",
      "\n",
      "for i in xrange (-7,8):    \n",
      "    LogReg = LogisticRegression(C=10**i)\n",
      "    LogReg.fit(ok_data_training, ok_labels_training)\n",
      "    a = LogReg.score(ok_data_testing, ok_labels_testing)\n",
      "    #Intercept (a.k.a. bias) added to the decision function. If fit_intercept is set to False, the intercept is set to zero.\n",
      "    bias = LogReg.intercept_\n",
      "    #Coefficient of the features in the decision function.\n",
      "    theta = LogReg.coef_\n",
      "    #y = 1/(1+e^-(xo+b))\n",
      "    accuracy = 1/(1+(np.exp(-np.dot(ok_data_testing, theta.T)-bias)))\n",
      "\n",
      "print 'Important information to re-use a trained classified in another system are bias and theta.'\n",
      "print 'New accuracy is', accuracy\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}