{
 "metadata": {
  "name": "",
  "signature": "sha256:b9e94bc311632494f56a6735381b1ef22b7d32c8ba031858f2e64c12e6ba0059"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import scipy.spatial.distance as dist\n",
      "from sklearn.metrics import accuracy_score\n",
      "from collections import Counter\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "import pylab as pl\n",
      "from sklearn.svm import LinearSVC, SVC"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "K-Nearest Neighbors"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Q1 Implement a function that does kNN classification, and use it to classify the Iris dataset. Once you get a near-perfect classification there, use your function in the 3DClothing dataset. Plot the accuracy for all odd values of k from 1 to 9.\n",
      "\n",
      "        Hint: the cdist function in SciPy may be helpful.\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = np.array([map(float, x.split(',')[:-1]) for x in open('iris.data') if x.strip()!=''])\n",
      "labels = np.array([x.split(',')[-1].strip() for x in open('iris.data') if x.strip()!=''])\n",
      "\n",
      "idx_train = np.loadtxt('iris_idx_train.txt')\n",
      "idx_test = np.loadtxt('iris_idx_test.txt')\n",
      "\n",
      "idx_train = idx_train.astype(int)\n",
      "idx_test = idx_test.astype(int)\n",
      "\n",
      "data_training = data[idx_train,:]\n",
      "data_testing = data[idx_test,:]\n",
      "labels_training = labels[idx_train]\n",
      "labels_testing = labels[idx_test]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def kNN(data_training, data_testing, labels_training, labels_testing):\n",
      "    for i in range(1,9):\n",
      "        distance = dist.cdist(data_training, data_testing)\n",
      "        min_k = np.argsort(distance.T,1)[:,1:i+1]\n",
      "        min_labels = labels_training[min_k]\n",
      "        accuracy = accuracy_score(labels_testing, [Counter(x).most_common()[0][0] for x in min_labels] )\n",
      "        print accuracy\n",
      "\n",
      "kNN (data_training, data_testing, labels_training, labels_testing)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Logistic Regression"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Q2 First we will focus on a two-class problem. Use slicing to get a new training and testing set that only contains the instances corresponding to shirt and jeans (remember to also create new label variables!). Then, train a Logistic Regression classifier, adjusting the C parameter with cross-validation. This time you can chose to use the cross-validation functions provided by sklearn. Plot the training and validation accuracy as C is incresed, and print the test accuracy for the selected model.\n",
      "\n",
      "    Let your C search range be from 10^-7 to 10^7.\n",
      "    Hint: use log-scale for the C value in the plot. Hint: Since we do not have a lot of training data, use 15 folds to ensure the train set will be large enough.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "A = list(range(1, 16))\n",
      "B = list(range(1, 16))\n",
      "p = 0\n",
      "data_training = np.load('3dclothing_train.npy')\n",
      "data_testing = np.load('3dclothing_test.npy')\n",
      "\n",
      "labels_training = np.array([x.strip() for x in open('3dclothing_labels_train.txt')])\n",
      "labels_testing = np.array([x.strip() for x in open('3dclothing_labels_test.txt')])\n",
      "\n",
      "ok_data_training = data_training[(labels_training == 'shirt') + (labels_training == 'jeans'),:]\n",
      "ok_data_testing = data_testing[(labels_testing == 'shirt') + (labels_testing == 'jeans'),:]\n",
      "\n",
      "ok_labels_training = labels_training[(labels_training == 'shirt') + (labels_training == 'jeans')]\n",
      "ok_labels_testing = labels_testing[(labels_testing == 'shirt') + (labels_testing == 'jeans')]\n",
      "\n",
      "for i in xrange (-7,8):    \n",
      "    LogReg = LogisticRegression(C=10**i)\n",
      "    A[p]=i\n",
      "    LogReg.fit(ok_data_training, ok_labels_training)\n",
      "    B[p] = LogReg.score(ok_data_testing, ok_labels_testing)\n",
      "    print 'Logistic Regression accuracy for C = 10^', i, ' is ', B[p]\n",
      "    p+=1\n",
      "\n",
      "pl.plot(A[:14],B[:14])\n",
      "\n",
      "pl.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Q4 Sometimes we want to re-use a trained classified in another system, or with another programming language. Other times we just want to save it to disk for later usage. Given a trained linear classifier object, explain what information should we be saving in order to be able to do so. Next, inspect the \"shirts vs jeans\" logistic regression classifier object we have trained in Question 2 (re-train it if necessary) and identify which variables contain said information. Finally, write the code necessary to use these variables to classify new test samples.\n",
      "\n",
      "    Hint: In Python, you can see the methods and variables of an object with dir(object).\n",
      "    Hint: Variables that start and end with two underscores, such as \"__dir__\" are internal Python object methods, not relevant for our problem.\n",
      "    Hint: In sklearn, variables tend to have a underscore at the end, while functions have no underscores at the beginning/ending of the name. Remember that the equation for Logistic Regression is:\n",
      "    logreg\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_training = np.load('3dclothing_train.npy')\n",
      "data_testing = np.load('3dclothing_test.npy')\n",
      "\n",
      "labels_training = np.array([x.strip() for x in open('3dclothing_labels_train.txt')])\n",
      "labels_testing = np.array([x.strip() for x in open('3dclothing_labels_test.txt')])\n",
      "\n",
      "ok_data_testing = data_testing[(labels_testing == 'shirt') + (labels_testing == 'jeans'),:]\n",
      "ok_data_training = data_training[(labels_training == 'shirt') + (labels_training == 'jeans'),:]\n",
      "\n",
      "ok_labels_training = labels_training[(labels_training == 'shirt') + (labels_training == 'jeans')]\n",
      "ok_labels_testing = labels_testing[(labels_testing == 'shirt') + (labels_testing == 'jeans')]\n",
      "\n",
      "for i in xrange (-7,8):    \n",
      "    LogReg = LogisticRegression(C=10**i)\n",
      "    LogReg.fit(ok_data_training, ok_labels_training)\n",
      "    a = LogReg.score(ok_data_testing, ok_labels_testing)\n",
      "    #Intercept (a.k.a. bias) added to the decision function. If fit_intercept is set to False, the intercept is set to zero.\n",
      "    bias = LogReg.intercept_\n",
      "    #Coefficient of the features in the decision function.\n",
      "    theta = LogReg.coef_\n",
      "    #y = 1/(1+e^-(xo+b))\n",
      "    accuracy = 1/(1+(np.exp(-np.dot(ok_data_testing, theta.T)-bias)))\n",
      "\n",
      "print 'Important information to re-use a trained classified in another system are bias and theta.'\n",
      "print 'New accuracy is', accuracy\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Support Vector Machines"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Q5 For this exercise we will be using the synthetic dataset jain. This dataset has only two dimensions, and therefore can be easily visualized. Train a linear Support Vector Machine (sklearn.svm.LinearSVC) and a SVM with a Gaussian Radial Basis Function Kernel (sklearn.svm.SVC) using the jain data (randomly split it 50-50 for train/test). Then, use the following Python function to visualize the decision boundaries of the two classifiers. Describe what you see."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Adapted from: Gael Varoquaux, Andreas Muller; \"Classifier comparison\"  \n",
      "#http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html  \n",
      "\n",
      "def paint_decision_functions(data, labels, clf):  \n",
      "    from matplotlib.colors import ListedColormap  \n",
      "    import pylab  \n",
      "    cm = pylab.cm.RdBu  \n",
      "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])  \n",
      "    x_min, x_max = data[:, 0].min() - .5, data[:, 0].max() + .5  \n",
      "    y_min, y_max = data[:, 1].min() - .5, data[:, 1].max() + .5  \n",
      "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),  \n",
      "                         np.arange(y_min, y_max, 0.1))  \n",
      "    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])  \n",
      "    Z = Z.reshape(xx.shape)  \n",
      "    pylab.contourf(xx, yy, Z, cmap=cm, alpha=.8)  \n",
      "    pylab.scatter(data[:, 0], data[:, 1], c=labels, cmap=cm_bright)  \n",
      "    pylab.xlim(xx.min(), xx.max())  \n",
      "    pylab.ylim(yy.min(), yy.max())  \n",
      "    pylab.xticks(())  \n",
      "    pylab.yticks(())  \n",
      "    pylab.show()  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = np.loadtxt('jain.txt')\n",
      "\n",
      "all_data = data[:,:-1]\n",
      "labels = data[:,-1]\n",
      "\n",
      "l = len(data)/2\n",
      "\n",
      "#Randomly permute a sequence, or return a permuted range.\n",
      "x = np.random.permutation(data.shape[0])\n",
      "\n",
      "data_training = all_data[x[:l],:]\n",
      "data_testing = all_data[x[:l],:]\n",
      "\n",
      "labels_training = labels[x[:l]]\n",
      "labels_testing = labels[x[:l]]\n",
      "\n",
      "y = LinearSVC()\n",
      "y.fit(data_training, labels_training)\n",
      "a = y.score(data_testing, labels_testing)\n",
      "print 'Support Vector Machine: ', a\n",
      "\n",
      "\n",
      "yg = SVC()\n",
      "yg.fit(data_training, labels_training)\n",
      "b = yg.score(data_testing, labels_testing)\n",
      "print 'SVM with a Gaussian Radial Basis Function Kernel: ', b \n",
      "\n",
      "paint_decision_functions(data_testing, labels_testing, clf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Classifier Evaluation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Q8 After developing a french fries image logistic regression classifier for our robot waiter, we want to know how good it is. We have run the image classifier in 150.000 images, of which 150 are positives. Load the probability of french fries computed by our classifier and the true labels, and compute:\n",
      "\n",
      "    Accuracy and Error Rate (assume threshold is at 0.5)\n",
      "    Balanced Error Rate (implies computing True Positive Rate, True Negative Rate)\n",
      "\n",
      "    F1-score (implies computing also precision and recall)\n",
      "\n",
      "        Hint: Since we will consider the whole dataset, there is no need to sort the values.\n",
      "\n",
      "    Since being a bit slower is prefearable to our robot attacking a client wearing stripes, we are more concerned about precision than recall. Compute the f-beta score with beta=0.5\n",
      "\n",
      "    Finally, plot the precision-recall curve for our classifier.\n",
      "\n",
      "        Use the function precision_recall_curve found in sklearn.metrics."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}